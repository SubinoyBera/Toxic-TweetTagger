# Toxic-Tweet Tagger

## â­ Overview :

<u><b>End to End Production-Grade Hate Speech Detection Application</b></u>

Unlike <b>typical ML Demos</b>, this project aims to represent <b>real-world production level workflows!</b> It covers the <b>complete lifecycle</b> from developing <b>core ML pipeline</b>, implementing <b>MLOps principles</b>, creating <b>inference API</b>, including <b>automated testing, experimentation tracking, CI/CD</b> and much more! Built to demonstrate how industries deploy ML solutions at scale, - ensuring <b>scalibility</b>, <b>reproducibility</b> and <b>maintainablity</b>. <br>
In essence, its <b>not just about building a machine learning model</b> with some fancy accuracy â€” it is about building a <b>complete ecosystem</b> around the model. It demonstrates how to design a ML system following the best practices used by ML teams and Engineers in <b>production environments</b>.

## ğŸš€ Highlights :

- ğŸ“‚ <b>End-to-End ML Pipeline</b> â€“ Complete ML lifecycle starting from data ingestion and preprocessing to model training, evaluation. Every stage is modular and reusable, making the pipeline easy to extend or adapt to new datasets.

- âš“ <b>Data Drift check with Evidently</b> â€“ Right after data ingestion, Evidently is used for data validation to ensure data quality, schema consistency, and distribution checks. This helps catch potential issues early and ensures only valid data flows into the training pipeline.

- ğŸ”— <b>Data & Pipeline Tracking with DVC</b> â€“ Datasets, artifacts and pipeline all are tracked with DVC, providing transparency, version-control and detect changes in any data or pipeline. Prevents recomputation if no changes is detected.

- ğŸ” <b>Experiment Tracking, Model Versioning & Registration in MLflow</b> â€“ Every training run is logged with metrics, hyperparameters, artifacts, and model versions, ensuring that results are fully reproducible and enabling easy comparison of experiments. Models are also saved in model registry with stages for reproducibility and identification which model is currently at production.

- ğŸ§ª <b>Automated Testing & Code Quality Checks</b> â€“ The project is production-hardened with unit tests, integration tests, and linting/formatting tools. CI ensures that code remains clean, reliable, and maintainable at scale.

- âŒ› <b>CI/CD Automation with GitHub Actions</b> â€“ The pipeline is integrated with continuous integration and deployment, enabling automatic testing, model retraining, and deployment with every new update, following true MLOps principles.

- âš¡ <b>FastAPI Inference Service</b> â€“ A high-performance REST API built with FastAPI for near real-time predictions. Designed with scalability in mind- it provides the production-ready serving layer that is containerized and deployed in HuggingFace Spaces.

- ğŸ”­ <b>Model Explainability with LIME</b> â€“ Integrated LIME (Local Interpretable Model-agnostic Explanations) to interpret individual predictions and understand why the model made a decision. This improves transparency, trust, and debuggability of the deployed ML system.

- ğŸ–¥ï¸ <b>Monitoring-ready Architecture</b> â€“ Used logging and exception modules for proper tracking and error handling. While Evidently is already used at data validation, the system is designed to extend monitoring into production with prometheus and grafana, continuous drift detection, performance tracking, and retraining triggers.

- ğŸ§© <b>Scalable, Modular and Industry-Grade Design</b> â€“ Developed and engineered following best practices from modern ML teams, ensuring the project is maintainable, extensible, and future-proof, going far beyond typical portfolio ML demos.

## ğŸ› ï¸ Tech Stack :

- <b>Languages</b> : Python, Jupyter | HTML, CSS (for frontend)
- <b>ML /DL/ NLP</b> : Pandas, numpy, scipy, scikit-learn, nltk, tensorflow
- <b>MLOps Tools</b> : Github Actions, Docker, MLflow, DVC, Evidently
- <b>Database Store</b> : MongoDB Atlas
- <b>API & Serving</b> : FastAPI, HuggingFace spaces (model inference server)
- <b>Testing & Code Quality</b> : tox, pytest, black, flake8, mypy
- <b>Deployment</b> : Vercel

## ğŸ¯ Application Preview :

<p>
    <img src="images/app_preview.png" alt="Application image" height="300" width="600">
</p>

## ğŸ”— Links :

- ### [Visit App â†—ï¸](https://toxictweet-tagger.vercel.app)
- ### [Hugging Face ğŸ¤—](https://huggingface.co/spaces/Subi003/ToxicTagger_serveAPI/tree/main)

ğŸ“Œ <b>PLEASE NOTE</b> :
This project focuses on Machine Learning engineering, MLOps, and model deployment aspects rather than full-stack web development.
The application interface is built using basic HTML & CSS only, just enough to demonstrate how end users can interact with the deployed model.
<br> The core emphasis of this work is on:

* Data pipeline design.
* Model versioning & experiment management with MLflow.
* MLOps best practices (CI/CD, testing, observability, deployment)

Used LIME for model observability, which increased inference time to more than 5 secs! Performed optimization and reduce it arround 0.7 secs. Still it can have latency issues and cold start delays upto 10 secs.<br>
<b> ğŸ“ All open source tools and free services have been used to develop this application!! ğŸ™âœ¨</b>
<br>

## ğŸ“Š Experiment Results :

<p>
    <img src="images/vizualization1.png" alt="Pair-plot visualization" height="300" width="600" style="margin-bottom: 10px;">
    <img src="images/data_drift-report.png" alt="MLflow Experiment Plot" height="300" width="600" style="margin-bottom: 10px;">
    <img src="images/all_experiments.png" alt="MLflow Experiment Plot" height="300" width="600" style="margin-bottom: 10px;">
    <img src="images/lime_report.png" alt="MLflow Experiment Plot" height="300" width="600">
    </p>
</p>

## âš™ï¸ Installation :

STEP: 01 - Clone this repository

```cmd
git clone https://github.com/SubinoyBera/ToxicTweet-Tagger
cd ToxicTweet-Tagger
```

STEP: 02 - Create and activate conda evironment

```bash
conda create -p venv python=3.11.5 -y
conda activate venv
```

STEP: 03 - Install project requirements. <br>
For running the application, only install app requiremnts

```bash
pip install -r app/requirements.txt
```

For development or running ML pipeline, install the full dev requirements

```bash
pip install uv  # Using uv for fast download [Optional]
uv pip install -r requirements-dev.txt
```

STEP: 04 - Build the package

```bash
python -m build
```

STEP: 05 - If you want to try out the ML pipeline then only perform this step, else please `skip this` and directly go to `STEP: 06`. <br>
Create a `.env` file in the root directory and add the required environment variables as mentioned in the given `.env.example` file. Make sure you have accounts in Dagshub, MongoDB Atlas and HuggingFace. Get your hate-tweet dataset, perform EDA, experiments and upload it to MongoDB. Update the files in the `src/` folder with your implementation and model parameters in the `params.yaml` file. Run this command to execute your ML pipeline :

```bash
dvc repro
```

STEP: 06 - Run the Application

```bash
python run_app.py
```

Open your browser and open the local URL : `http://localhost:8000/`.<br>
Write your comment and click on `predict` button. To view the full respose from the model inference API click on the link shown below the prediction result: `view API response`

<hr>

### ğŸ’¥ Upcoming Update :

Event Driven System Architecture with Kafka backbone for scalability, high performance and low latency with monitoring using prometheus and grafana. Also improving both model and system performance overall for creating a robust ML system.
<br>

ğŸ—ï¸ğŸ™ **THANK YOU !!** :) <br>
<b>-_with love & regards : Subinoy Bera (developer)_</b><br>
ğŸ§¡ğŸ¤ğŸ’š
